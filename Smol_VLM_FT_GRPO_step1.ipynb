{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaodongeast/multimodal/blob/main/Smol_VLM_FT_GRPO_step1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc0g2NLpUSGr"
      },
      "source": [
        "# adpate from this official notebook for the sft\n",
        "Fine-tune SmolVLM on Visual Question Answering using Consumer GPU with QLoRA\n",
        "\n",
        "In this notebook we will fine-tune SmolVLM VQAv2 dataset. With this notebook you can also fine-tune Idefics3, since both models have the same model class/architecture.\n",
        "\n",
        "We will use some techniques in this notebook that will let you fine-tune the model on L4 with batch size of 4 only using around 16.4 GB of VRAM. We ran this notebook in that setup to test, but because we were able to afford A100 this notebook was last ran on an A100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIhA1lQ7j0kw"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate datasets peft bitsandbytes tensorboard\n",
        "!pip install -q flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAeMA0heVBjT"
      },
      "source": [
        "We will push out model to Hub so we need to authenticate ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRs44KbCZeZ7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive6')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNgbe7YEmRZW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import concatenate_datasets, DatasetDict\n",
        "from huggingface_hub import notebook_login\n",
        "import torch\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics3ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRq8ve-LVAzU"
      },
      "source": [
        "In this notebook we will not do full fine-tuning but use QLoRA method, which loads an adapter to the quantized version of the model, saving space. If you want to do full fine-tuning, set `USE_LORA` and `USE_QLORA` to False. If you want to do LoRA, set `USE_QLORA`Â to False and `USE_LORA`Â to True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9CDMq0duYYn"
      },
      "outputs": [],
      "source": [
        "USE_LORA = True\n",
        "USE_QLORA = False\n",
        "SMOL = True\n",
        "\n",
        "model_id = \"HuggingFaceTB/SmolVLM-instruct\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_id\n",
        ")\n",
        "\n",
        "if USE_QLORA or USE_LORA:\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=8,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n",
        "        use_dora=False if USE_QLORA else True,\n",
        "        init_lora_weights=\"gaussian\"\n",
        "    )\n",
        "    lora_config.inference_mode = False\n",
        "    if USE_QLORA:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "\n",
        "    model = Idefics3ForConditionalGeneration.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config if USE_QLORA else None,\n",
        "       # _attn_implementation=\"flash_attention_2\",\n",
        "        device_map=\"cuda\",\n",
        "        dtype=torch.bfloat16\n",
        "    )\n",
        "    model.add_adapter(lora_config)\n",
        "    model.enable_adapters()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    print(model.get_nb_trainable_parameters())\n",
        "else:\n",
        "    model = Idefics3ForConditionalGeneration.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        _attn_implementation=\"flash_attention_2\",\n",
        "        dtype=torch.bfloat16 # Add dtype for non-QLoRA\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # if you'd like to only fine-tune LLM\n",
        "    for param in model.model.vision_model.parameters():\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIVhpp0EyZO2"
      },
      "source": [
        "The model as is is holding 2.7 GB of GPU RAM ðŸ’—"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMTtg3dl3NX2"
      },
      "source": [
        "##Â Loading the dataset and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWHMWTSZ3Pyr"
      },
      "source": [
        "We will load a small portion of the VQAv2 dataset. We are loading a small portion of the model for education purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POOqKqYRka5O"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Znf9vMo5rnSd"
      },
      "outputs": [],
      "source": [
        "#split_ds = ds[\"validation\"].train_test_split(test_size=0.9)\n",
        "#train_ds = split_ds[\"train\"]\n",
        "\n",
        "\n",
        "#notebook_login()\n",
        "\n",
        "ds = load_dataset('bugkiller2025/train')\n",
        "\n",
        "\n",
        "# merge all splits into one big dataset\n",
        "full = concatenate_datasets([split for split in ds.values()])\n",
        "\n",
        "# shuffle once\n",
        "full = full.shuffle(seed=42)\n",
        "\n",
        "# now split: 50% train vs 50% remaining\n",
        "train_test = full.train_test_split(test_size=0.6, seed=42)\n",
        "train_ds = train_test[\"train\"]\n",
        "remaining = train_test[\"test\"]\n",
        "\n",
        "# split remaining into eval (10%) and test (10%) of total\n",
        "intermediate = remaining.train_test_split(test_size=0.6, seed=42)\n",
        "eval_test = intermediate[\"train\"].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "eval_ds = eval_test[\"train\"]   # ~25% of total\n",
        "test_ds = eval_test[\"test\"]    # ~25% of total\n",
        "\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": train_ds,\n",
        "    \"eval\": eval_ds,\n",
        "    \"test\": test_ds\n",
        "})\n",
        "print(dataset_dict)\n",
        "\n",
        "dataset_dict.push_to_hub(\"bugkiller2025/vqa_reasoning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW0RV6aW1GHF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import concatenate_datasets, DatasetDict\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "#notebook_login()\n",
        "\n",
        "ds = load_dataset(\"bugkiller2025/vqa_reasoning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sghQW4h2m1U6"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIDioFlRuYYn"
      },
      "outputs": [],
      "source": [
        "train_ds = ds['train']\n",
        "eval_ds = ds['eval']\n",
        "test_ds = ds['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHJYgdcive0n"
      },
      "outputs": [],
      "source": [
        "print(train_ds[0])\n",
        "train_ds[0]['image']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdnjriZy2cGG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGaGJcYcwV1o"
      },
      "outputs": [],
      "source": [
        "from transformers import Idefics3ForConditionalGeneration,AutoProcessor\n",
        "import torch\n",
        "\n",
        "#model_id =   \"HuggingFaceTB/SmolVLM-instruct\"\n",
        "#model = Idefics3ForConditionalGeneration.from_pretrained(\n",
        " #       model_id,\n",
        "  #  ).to('cuda')\n",
        "\n",
        "#processor = AutoProcessor.from_pretrained(\n",
        " #   \"HuggingFaceTB/SmolVLM-instruct\"\n",
        "#)\n",
        "\n",
        "# Use a consistent instruction; put it in the system message.\n",
        "instruct =\"\"\"Answer question about the image with your reasoning. Follow this format:\n",
        "<think>\n",
        "[Your detailed chain-of-thought goes here]\n",
        "</think>\n",
        "<answer>\n",
        "[Your final answer goes here]\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from os import system\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "from PIL import Image\n",
        "\n",
        "print(processor.tokenizer.SPECIAL_TOKENS_ATTRIBUTES)\n",
        "print(processor.tokenizer.additional_special_tokens)\n",
        "\n",
        "def _normalize_example(example: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Converts your record into: question(str), answer_text(str), image(PIL), solution(str or \"\")\n",
        "    Supports:\n",
        "      - answer as index into choices\n",
        "      - or 'multiple_choice_answer' already as string\n",
        "      - solution may be missing (defaults to \"\")\n",
        "    \"\"\"\n",
        "    # --- image ---\n",
        "    img = example[\"image\"]\n",
        "    if isinstance(img, dict) and \"path\" in img:\n",
        "      img = Image.open(img[\"path\"]).convert(\"RGB\")\n",
        "    elif hasattr(img, \"mode\"):\n",
        "        if img.mode != \"RGB\":\n",
        "            img = img.convert(\"RGB\")\n",
        "    else:\n",
        "        # last resort: if it's a path string\n",
        "        img = Image.open(str(img)).convert(\"RGB\")\n",
        "    # --- question ---\n",
        "    question = example.get(\"question\", \"\").strip()\n",
        "\n",
        "    # --- answer text ---\n",
        "    if \"multiple_choice_answer\" in example and example[\"multiple_choice_answer\"] is not None:\n",
        "        answer_text = str(example[\"multiple_choice_answer\"]).strip()\n",
        "    else:\n",
        "        # Your format: choices + integer answer index\n",
        "        choices = example.get(\"choices\")\n",
        "        ans_idx = example.get(\"answer\")\n",
        "        if isinstance(ans_idx, int) and isinstance(choices, list) and 0 <= ans_idx < len(choices):\n",
        "            answer_text = str(choices[ans_idx]).strip()\n",
        "        else:\n",
        "            # fallback: maybe \"answer\" is already a string\n",
        "            answer_text = str(example.get(\"answer\", \"\")).strip()\n",
        "\n",
        "    # --- chain-of-thought / solution ---\n",
        "    thought = str(example.get(\"solution\", \"\") or \"\").strip()\n",
        "\n",
        "    return {\n",
        "        \"image\": img,\n",
        "        \"question\": question,\n",
        "        \"answer_text\": answer_text,\n",
        "        \"thought\": thought,\n",
        "        \"choices\": example.get(\"choices\")  # passthrough (optional)\n",
        "    }\n",
        "\n",
        "def get_response(example, processor, model):\n",
        "  device ='cuda'\n",
        "  image = example[\"image\"]\n",
        "  # if decode=False, you'll get {'path': '...', 'bytes': None}\n",
        "\n",
        "  question = example[\"question\"]\n",
        "\n",
        "  messages = [\n",
        "\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\":instruct + '\\n' + 'Querstion:\\n' + question}\n",
        "\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "  formatted_query = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "  print(formatted_query)\n",
        "\n",
        "  # Tokenize the query\n",
        "  model_inputs = processor(\n",
        "      images=image,\n",
        "      text=formatted_query,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(device, dtype=torch.bfloat16) # Add dtype here\n",
        "\n",
        "  # Generate predictions\n",
        "  with torch.no_grad():\n",
        "      outputs = model.generate(**model_inputs,max_new_tokens =200)\n",
        "  trimmed_generated_ids = [out_ids[len(in_ids) :] for in_ids, out_ids in zip( model_inputs.input_ids, outputs)]\n",
        "\n",
        "\n",
        "  # Decode the prediction\n",
        "  prediction = processor.batch_decode(  trimmed_generated_ids, skip_special_tokens=True)\n",
        "\n",
        "  # Display the result\n",
        "  print(f\"Query: {question}\")\n",
        "\n",
        "  print(f\"Expected Answer: {example['answer_text']}\")\n",
        "  print(\"=\"*20)\n",
        "  print(f\"Model Prediction: {prediction}\")\n",
        "\n",
        "\n",
        "test_sample = _normalize_example(ds['train'][1])\n",
        "print(test_sample)\n",
        "get_response(test_sample, processor, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWQ738V03PN-"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sample = _normalize_example(ds['test'][110])\n",
        "print(test_sample)\n",
        "get_response(test_sample, processor, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmNPx5pdy3GX"
      },
      "outputs": [],
      "source": [
        "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
        "            processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
        "print(processor.tokenizer.SPECIAL_TOKENS_ATTRIBUTES)\n",
        "print(processor.tokenizer.additional_special_tokens)\n",
        "\n",
        "\n",
        "from os import system\n",
        "from pprint import pprint\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "from PIL import Image\n",
        "\n",
        "def format_example(exn: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Builds chat-style inputs for SmolVLM where the assistant replies with:\n",
        "        <think> ... </think>\\n<answer> ... </\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Comment this block out if you don't want choices shown to the model.\n",
        "    q = exn[\"question\"]\n",
        "    #if isinstance(exn.get(\"choices\"), list) and exn[\"choices\"]:\n",
        "        # e.g., \"Question ...\\nOptions: yes | no\"\n",
        "    #    q = f\"{q}\\nOptions: \" + \" | \".join(map(str, exn[\"choices\"]))\n",
        "\n",
        "    # Desired assistant output:\n",
        "    #   <think> ...</think>\n",
        "    #   <answer> ...</answer>\n",
        "    question = exn[\"question\"]\n",
        "\n",
        "\n",
        "    thoughts = exn['thought'] if exn['thought'] else \"\"\n",
        "    assistant_text = f\"\"\"<think>\n",
        "    {thoughts.strip()}\n",
        "    </think>\n",
        "    <answer>\n",
        "    {exn['answer_text'].strip()}\n",
        "    </answer>\"\"\"\n",
        "\n",
        "    messages = [\n",
        "      {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\":instruct + '\\n' + 'Querstion:\\n' + question}\n",
        "\n",
        "            ],\n",
        "        },\n",
        "\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": assistant_text}\n",
        "            ]\n",
        "        }\n",
        "      ]\n",
        "    return messages\n",
        "\n",
        "pprint(_normalize_example(ds['train'][0]))\n",
        "#print(\"=\" * 100)\n",
        "exn = _normalize_example(ds['train'][0])\n",
        "pprint(format_example(exn))\n",
        "\n",
        "\n",
        "def collate_fn(examples):\n",
        "  texts = []\n",
        "  images = []\n",
        "  for example in examples:\n",
        "      exn = _normalize_example(example)\n",
        "      image = exn[\"image\"]\n",
        "      messages = format_example(exn)\n",
        "      #print(messages)\n",
        "      text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
        "      #print(text)\n",
        "      texts.append(text)\n",
        "      images.append([image])\n",
        "\n",
        "  batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "  labels = batch[\"input_ids\"].clone()\n",
        "  labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "  labels[labels == image_token_id] = -100\n",
        "  batch[\"labels\"] = labels\n",
        "\n",
        "  return batch\n",
        "\n",
        "_ = collate_fn([train_ds[2]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nwMO3n0X7Hv"
      },
      "source": [
        "Let's write our data collating function. We will apply prompt template to have questions and answers together so model can learn to answer. Then we pass the formatted prompts and images to the processor which processes both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0krVLZ-wNMl"
      },
      "outputs": [],
      "source": [
        "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
        "            processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
        "print(processor.tokenizer.SPECIAL_TOKENS_ATTRIBUTES)\n",
        "print(processor.tokenizer.additional_special_tokens)\n",
        "def collate_fn(examples):\n",
        "  texts = []\n",
        "  images = []\n",
        "  for example in examples:\n",
        "      example = _normalize_example(example)\n",
        "\n",
        "      messages = format_example(example)\n",
        "      image = example[\"image\"]\n",
        "      text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
        "      texts.append(text.strip())\n",
        "      images.append([image])\n",
        "\n",
        "  batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "  labels = batch[\"input_ids\"].clone()\n",
        "  labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "  labels[labels == image_token_id] = -100\n",
        "  batch[\"labels\"] = labels\n",
        "\n",
        "  return batch\n",
        "\n",
        "_ = collate_fn([train_ds[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEYDjWpE3LD5"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvAs896cdwg8"
      },
      "source": [
        "We can now initialize `Trainer`Â and initialize `TrainingArguments`Â to pass to `Trainer`.\n",
        "\n",
        "Some notes:\n",
        "- If you use 8-bit QLoRA with the below setup it uses around 16.4 GB VRAM (beautiful, fits comfortably inside L4, Colab free tier)\n",
        "- We use gradient accumulation to simulate a larger batch size.\n",
        "- We also save up on memory from intermediate activations by using gradient checkpointing.\n",
        "\n",
        "**Disclaimer:**\n",
        "The techniques here aren't free lunch. The latter two will add additional compute to the training, thus slow down a bit (for reference on two A100s with bsz of 16, we were able to train for 2 hrs 43 mins with the gradient accumulation steps of 4, disabling it reduced it with 2 hr 35 mins).\n",
        "If you want to speed-up, you might play around, reduce to 4-bit precision and have a higher batch size. Note that 4-bit might result in model learning less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNE2yWAYrAhD"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "model_name = model_id.split(\"/\")[-1]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=8,\n",
        "    warmup_steps=10,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=20,\n",
        "    save_total_limit=1,\n",
        "    optim=\"paged_adamw_8bit\", # for 8-bit, keep this, else adamw_hf\n",
        "    bf16=True, #Â underlying precision for 8bit\n",
        "    output_dir=f\"/content/drive6/MyDrive/smolvlm-instruct-s4\",\n",
        "    hub_model_id=f\"smolvlm-instruct-s4\",\n",
        "    report_to=\"tensorboard\",\n",
        "    remove_unused_columns=False,\n",
        "    gradient_checkpointing=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBBSDpBhreJd"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    train_dataset=train_ds,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_QOCpw_-uYYo"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.save_model(\"/content/drive6/MyDrive/smolvlm-instruct-s4\")\n",
        "trainer.push_to_hub(\"smolvlm-instruct-s4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hN0QD9_uYYo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhTGGj2D62L-"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3pUzTld6_Gz"
      },
      "outputs": [],
      "source": [
        "load_id = f'bugkiller2025/{model_name}-vqav2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaTp3NmT7FMb"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(f\"{model_name}-vqav2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOsHnL4C8cW8"
      },
      "outputs": [],
      "source": [
        "model_no_train = Idefics3ForConditionalGeneration.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        _attn_implementation=\"flash_attention_2\",\n",
        "    ).to('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqu1L2B77QsG"
      },
      "outputs": [],
      "source": [
        "model = Idefics3ForConditionalGeneration.from_pretrained(\n",
        "        load_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        _attn_implementation=\"flash_attention_2\",\n",
        "    ).to('cuda')\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVcB9Von8ETA"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sample = _normalize_example(test_ds[1])\n",
        "print(test_sample)\n",
        "get_response(test_sample, processor, model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UY0GaCu9WDK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpeWEBHe9Wfr"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sample = _normalize_example(test_ds[1])\n",
        "print(test_sample)\n",
        "get_response(test_sample, processor, model_no_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkVmdBp18KlQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ebpWJMx67Iq"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo5rMv3-6zo9"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}